

SUMYAK JAIN
500068360, R171218106



 





APPLICATION AND CONTAINERIZATION LAB FILE


1.Installing Vagrant & Creating basic vagrant box using VirtualBox virtualization &
2.Understanding vagrant file - Configuration - CPU, RAM, Storage, Provisioning (Shell Script).
1)	download vagrant
 

2)	check vagrant version
 

3)	create a folder and then go to that folder and do vagrant init
 





4)	open vagrantfile and do the changes in “config.vm.box = "ubuntu/trusty64"

 

5)	run vagrant up command
 

6)	run vagrant SSH
 
 
7)	check the VM created
 


















3.Docker Machine - Installation, configuration, creating machines (on VirtualBox) &
4.Docker - Installation, Configuration, Running Images.
Install docker using 
$ sudo apt install docker.io
1. check docker version
 
2. see any running container
 
3. see all docker container run so far
 
4. make a container of alpine image
 









5.Dockerfile - Containerizing application, Building Images, Tagging,
publishing &
6.DTR - Docker Hub, Private Registries, Publishing images.

1)	Login to docker hub
 
2)	Write Dockerfile and index.html file in a particular location
 
Dockerfile
 
Index.html
 



3)	Build an image using Dockerfile
Docker build -t <image name>:<tag name> .
“.” (here “.” means Dockerfile in same location)
 
4)	Check for images
 
5)	Make a container using image id and check for the container is made or not
 



6)	For pushing an image to dockerhub we need to tag the image with dockerhub repo
docker tag <image id> [<user name>/<repo name>]:<tag name>
 
7)	Push the image to dockerhub
 
8)	Check for the image on dockerhub
 








9)	Make a container on some different container instance
 
10)	Attach a port no. and run the container to check the output
 

11)	Check the output on localhost:5000
 




7.Docker - Volumes, Env, Monitoring (Docker stats).
1) Checking data in container without docker volume attached and then attaching docker volume to the container
1.1 check docker version
 
1.2 see any running container
 
1.3 see all docker container run so far
 
1.4 make a container of alpine image
 

1.5      
•	go to root folder of alpine image i.e “mnt”
•	Create files a.txt, b.txt
•	And view files
•	The exit the container
 


1.6 see the container is exited
 

1.7 
•	make alpine container again
•	And go to its root folder and check for the files created before
(they are not there)


 

1.8 now make another container but this time with “docker volume”
And again
•	go to root folder of alpine image i.e “mnt”
•	Create files a.txt, b.txt
•	And view files
•	The exit the container
 
1.9 make another container and check for the files created before now this time files are present
Even after we have exited the container
 

2) Docker volume inspection

2.1 you can check all the volumes created
 

2.2 you can inspect your volume and know its location
 

2.3 you can go to your volume location and check for the content in it
 
2.4 removing docker volume
 


3) making container of same image with different names and sharing files between them

3.1 create a container naming c1 with same docker volume attached and see the files present in it
 
3.2 create another container with name c2 and same docker volume attached 
 
3.3 create a file c.txt in container c2
 
3.4 you see the same file c.txt in c1 container also
 

4) making container of different image and sharing files between them

4.1
•	Create another container but this time with different image “ubuntu” with same docker volume
•	Go to its root folder “var”
•	Create a file d.txt
•	And exit the container
 
4.2 check another container of different image have the same files present made on the another container with different image
 











8.Docker Compose - Installation, Creating Compose files, Running Images using docker-compose.
Docker Compose is a tool used to create a Docker application that will run on multiple containers. A YAML configuration file is used where the configuration of the containers is given.
In this experiment, we will create a two-service configuration with MySQL as database and Nginx image as web server.
The steps that need to be followed are:

1. Make sure that docker-compose is installed in your machine. If not, then install it as follows:
Command: sudo apt install docker compose
 

2. Make a .env file with the value of environment variables of MySQL as follows:
MYSQL_ROOT_PASSWORD=<password>
MYSQL_DATABASE=<database-name>
MYSQL_USER=<username>
Provide the values of these variables in the file.
 
As shown above, the values of environment variables is provided in the file.
This file will be used to refer to the value of specified variables.


3. Make a YAML configuration file with the name “docker-compose.yml” and provide the configuration of containers in this file.
Here, we are defining two services namely databases and web. Databases service uses mysql image container and port 3307 of the container is bind to port 3306 of the machine. The environment file created in the previous step is specified for the value of environment variables.
Web service uses nginx image container and port 80 of the container is bind to the port 80 of the machine. The web service depends on the databases service.
 















4. Run the services specified in the above files as follows:
Command: docker-compose up
You will see a similar output:
 
 

5. you can check your container using “docker ps-a”
  




9.Running Multi-Container applications using docker compose and on Swarm.
Docker Links:
1.	Open the terminal and run a postgres container by the command ‘docker run -it -d –name db training/postgres’.

2.	Once the command execution is complete and the pulling of the image gets over, run another container of image webapp linking it with the db container we just created by the command ‘docker run -it -d –name web –link db:mydb training/webapp’. 
 

3.	Once the other image is pulled completely and the container is running then switch to the terminal of the container. Command for logging in or switching to container terminal is ‘docker exec -it <container-name> bash’. Here the command would be ‘docker exec -it web bash’.

4.	After successfully logging in to the container terminal, ping the container with which we linked the current container and the ping command will run successfully.
 

5.	Now run another container without linking it to any of the previous containers and check all the containers with the command ‘docker ps’.

6.	Login to the container again with which we ran the ping command once and try pinging the newly created container, the ping command will fail because the container isn’t linked with the other container.

 

Docker Swarm:
1.	Open two terminals one for being the master and other for being the slave (node), check if swarm is active or not by the command ‘docker node ls’. If the swarm is active the command will list the master and the slaves but if the swarm isn’t active the command will throw an error.

2.	Activate the swarm using the command ‘docker swarm init’, the output of the command will have a command having unique token and the ip address. Through this command we can connect the other terminal as a worker to the master by running the command in the terminal.
Basic syntax of the command from the output is ‘docker swarm join –token <token> <ip and port no.>’

 


3.	Again run the command ‘docker node ls’ to view all the nodes in the swarm we just activated, the nodes which are acting as worker will be written directly but the node which is acting as a manager will have Leader written with it and a ‘*’ mark on it.

4.	Only the manager can access the information regarding the number of rows and nodes in the swarm so if we try to run the command in a worker node terminal, it will fail.

 
 









5.	We can check all the details of the swarm and the nodes by running the command ‘docker info’.
 

6.	When we want to remove a node from the swarm we use the command ‘docker swarm leave --force’ if the node is a manager and ‘docker swarm leave’ if it’s a worker node. If a manager leaves the swarm, the manager properties are automatically inherited by a worker node and it becomes the new manager.  
 






10.Kubernetes -Minikube installation and fundamentals. &
11.Deploying Pods and Services on minikube.
1)	Launch Cluster
minikube start --wait=false
kubectl get nodes
 
2)	Kubectl run
kubectl run http --image=katacoda/docker-http-server:latest --replicas=1
kubectl get deployments
kubectl describe deployment http
 

3)	Kubectl Expose
kubectl expose deployment http --external-ip="172.17.0.40" --port=8000 --target-port=80
curl http://172.17.0.40:8000
 

4)	Kubectl Expose
kubectl run httpexposed --image=katacoda/docker-http-server:latest --replicas=1 --port=80 --hostport=8001
curl http://172.17.0.40:8001
docker ps | grep httpexposed
 
         
5)	Scale Containers
kubectl scale --replicas=3 deployment http
     kubectl get pods  
curl http://172.17.0.40:8000
 
Launch Single Node Kubernetes Cluster

1)	Start Minikube
 
2)	Cluster info
 

3)	Deploy Containers
 

4)	Dashboard
 
 


















12.Build Docker Image using  .Dockerignore file
Create build without .dockerignore file
Password.txt is there
 
Making another build with password.txt in .dockerignore file
Now password.txt is not there in ls/app
 



In the environment, a 100M temporary file has been created. This file is never used by the Dockerfile. When you execute a build command, Docker sends the entire path contents to the Engine for it to calculate which files to include. As a result sending the 100M file is unrequired and creates a slower build.
 
Making the build again by putting big-temp-file.img to .dockerignore file to optimise the build size
 











13.Prepare and Implement Docker Container Restart Policy
The option --restart=on-failure:# allows you to say how many times Docker should try again. In the example below, Docker will restart the container three times before stopping.
docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example

Use the always flag to automatically restart the container when is crashes for example
docker run -d --name restart-always --restart=always scrapbook/docker-restart-example

 











14.Working with Metadata, Log File using Docker
MANAGE LOG FILES
By default, the Docker logs are outputting using the json-file logger meaning the output stored in a JSON file on the host. This can result in large files filling the disk. As a result, you can change the log driver to move to a different destination.
The command below will redirect the redis logs to syslog.
docker run -d --name redis-syslog --log-driver=syslog redis
The third option is to disable logging on the container. This is particularly useful for containers which are very verbose in their logging.
When the container is launched simply set the log-driver to none. No output will be logged.
docker run -d --name redis-none --log-driver=none redis
The inspect command allows you to identify the logging configuration for a particular container. The command below will output the LogConfig section for each of the containers.
Server created in step 1
docker inspect --format '{{ .HostConfig.LogConfig }}' redis-server
Server created in step 2
docker inspect --format '{{ .HostConfig.LogConfig }}' redis-syslog
Server created in this step
docker inspect --format '{{ .HostConfig.LogConfig }}' redis-none
 

METADATA
 
 
